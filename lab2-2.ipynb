{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint as pp\n",
    "import math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empezamos  inicializando los vectores Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definamos los tamaños de cada capa. La de input sera de 784, la hidden layer de 258, la output de 10 (existen 10 posibles clasificaciones).\n",
    "layer_sizes={'ilayer':28, 'hlayer':7, 'olayer':10}\n",
    "##Thetas\n",
    "#Solo tenemos una hidden layer, por lo que tenemos un theta de I a Z(1), y una de Z(1) a O.\n",
    "#Les generamos valores aleatorios entre 0 y 1.\n",
    "thetaI_Z = np.random.rand(layer_sizes['ilayer'], layer_sizes['hlayer'])\n",
    "thetaZ_O = np.random.rand(layer_sizes['hlayer'], layer_sizes['olayer'])\n",
    "thetas_dict = {'Theta I_Z': thetaI_Z,\n",
    "              'Theta Z_O': thetaZ_O}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definamos las funciones a utilizar.\n",
    "def sigmoid(Z):\n",
    "    #print (1/(1+np.exp(-Z)))\n",
    "    return (1/(1+np.exp(-Z)))\n",
    "def sigmoid_derivative(Z):\n",
    "    #print(Z, 1-Z)\n",
    "    return (Z)*(1-(Z))\n",
    "def forward_propagate(X, activation_function=sigmoid):\n",
    "    ##...hay una discusion por hacerse sobre utilizar diccionarios dinamicos para theta de tamaño variable..\n",
    "    ##..por ahora los haremos constantes\n",
    "    \n",
    "    #print(X)\n",
    "    #print(thetas_dict['Theta I_Z'])\n",
    "    Z_1 = np.dot(X, thetas_dict['Theta I_Z'])\n",
    "    Z_2 = activation_function(Z_1) ##Necesitamos una funcion de activacion, por default usaremos sigmoide.\n",
    "    #print(Z_2)\n",
    "    Z_3 = np.dot(Z_2, thetas_dict['Theta Z_O'])\n",
    "    #print(Z_3)\n",
    "    activation = sigmoid(Z_3)\n",
    "    results_dict = {'Z1': Z_1,\n",
    "                   'Z2': Z_2,\n",
    "                   'Z3': Z_3,\n",
    "                   'Activation': activation}\n",
    "    return results_dict\n",
    "\n",
    "def backprop(X, y, forward_prop_results, activation_derivative=sigmoid_derivative, debug=False, ld=0.0001):\n",
    "    ##Desenpaquemos valores\n",
    "    #Fprop, al final solo usamos estos dos, pero sirve tener lo otro para debug.\n",
    "    fprop_output = forward_prop_results['Activation']\n",
    "    fprop_Z2 = forward_prop_results['Z2']\n",
    "    #Thetas\n",
    "    #theta_1 = thetas_dict['Theta I_Z']\n",
    "    #theta_2 =  \n",
    "    ##Errores de salida\n",
    "    output_error = fprop_output-y\n",
    "    sigmodp_fo=activation_derivative(fprop_output)\n",
    "    output_smalldelta = output_error * sigmodp_fo\n",
    "    ##Errores de hLayer\n",
    "    hlayer_error = output_smalldelta.dot(thetas_dict['Theta Z_O'].T)\n",
    "    sigmoidp_hl = activation_derivative(fprop_Z2)\n",
    "    hlayer_smalldelta = hlayer_error* sigmoidp_hl*ld\n",
    "    if debug:\n",
    "        pass\n",
    "        #np.savetxt('deltas.txt', output_smalldelta)\n",
    "        #print(\"Smalldelta:\",hlayer_smalldelta, \"\\nError\", hlayer_error,\"\\nSprime\", sigmoidp_hl)\n",
    "        #print(\"Output Smalldelta:\",output_smalldelta, \"\\noError\", output_error,\"\\nSprime\", sigmodp_fo)\n",
    "    \n",
    "    ##Ajustes a theta\n",
    "    #Creemos un nuevo dict para no modificar el original (esto se ignorara si provoca problemas de memoria)\n",
    "    if debug:\n",
    "        print(\"ThetasIZ:\",thetas_dict['Theta I_Z'][1])\n",
    "        print(\"ThetasZO:\",thetas_dict['Theta Z_O'][1])\n",
    "        \n",
    "    #print(\"Before:\",thetas_dict['Theta I_Z'][1])\n",
    "    #print((np.amax(thetas_dict['Theta I_Z'])))\n",
    "    thetas_dict['Theta I_Z'] += (X.T.dot(hlayer_smalldelta))\n",
    "    thetas_dict['Theta Z_O'] += fprop_Z2.T.dot(output_smalldelta)\n",
    "    thetas_dict['Theta Z_O'] = thetas_dict['Theta Z_O']/(np.amax(thetas_dict['Theta Z_O']))\n",
    "    #print(\"After:\",thetas_dict['Theta I_Z'][1])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de capa de entrada\n",
    "En este caso utilizamos la suma de los valores de cada linea en la imagen para reducir la cantidad de neuronas en cada capa.\n",
    "Se comentaron todos los procesos que dependen del git de mnist porque no logro hacer que funcione en binder. En su lugar, cargo dos archivos que contienen la misma informacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definamos la neurona de entrada.\n",
    "#Mi idea es utilizar la suma de los valores de cada pixel en cada linea como mi input layer, reduciendo de 784 neuronas a 28.\n",
    "##Entonces cargamos nuestros datasets de entrenamiento\n",
    "#from utils import mnist_reader\n",
    "#X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "\n",
    "#X_train = X_train[:15000]\n",
    "#print(X_train)\n",
    "#X_train = X_train/(np.amax(X_train))\n",
    "#print(X_train)\n",
    "\n",
    "#Resheapeamos a y_train para que sea (60k, 1), si no da error.\n",
    "\n",
    "#y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "#y_train = y_train[:15000]\n",
    "#y_train_new = np.zeros((y_train.shape[0], 10))\n",
    "#for i in range(len(y_train_new)):\n",
    "#    val = y_train[i][0]\n",
    "#    y_train_new[i][val] = 1\n",
    "    \n",
    "#print(y_train_new)\n",
    "##Convertimos nuestro X_train de 784 columnas a un X_train_reduced de 28:\n",
    "#X_train_reduced = np.zeros((X_train.shape[0], 28))\n",
    "#for line in X_train:\n",
    "#    X_train_reduced[line] = (np.sum(line.reshape(-1, 28),axis=1))    \n",
    "\n",
    "X_train = np.loadtxt('X_train.txt')\n",
    "Y_train = np.loadtxt('Y_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probemos que hace nuestra funcion feed forward con lo que tenemos.\n",
    "#fprop_res = forward_propagate(X_train_reduced,  sigmoid)\n",
    "#backprop(X_train_reduced, y_train,  fprop_res, sigmoid_derivative, debug=True)\n",
    "#fprop_res = forward_propagate(X_train_reduced,  sigmoid)\n",
    "\n",
    "#print(thetas_dict['Theta Z_O'][1])\n",
    "#Funcionan, creo, entonces probemos repetirlo n veces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion para entrenar la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, iter=1000, verbose_div=1000,  activation=sigmoid, activation_derivative = sigmoid_derivative):\n",
    "    debug = True\n",
    "    for i in range(iter):\n",
    "        fprop_res = forward_propagate(X,  activation)\n",
    "        backprop(X, y,  fprop_res, sigmoid_derivative, debug)\n",
    "        debug = False\n",
    "        if (i % verbose_div == 0) or (i == iter) :\n",
    "            print(\"Expected output of first 10 samples...\\n\")\n",
    "            print(y[:10])\n",
    "            print(\"Obtained output of first 10 samples...\\n\")\n",
    "            print(fprop_res['Activation'])\n",
    "            print(\"Cost using mean sum squared:\\n\")\n",
    "            print(y - fprop_res['Activation'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X_train, Y_train, iter=1000,verbose_div=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos los sets de prueba con la misma forma que el de entrenamiento\n",
    "Se hizo lo mismo que en la parte de los train sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "#y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "#y_test_new = np.zeros((y_test.shape[0], 10))\n",
    "#for i in range(len(y_test_new)):\n",
    "#    val = y_test[i][0]\n",
    "#    y_test_new[i][val] = 1\n",
    "    \n",
    "\n",
    "#X_test_reduced = np.zeros((X_test.shape[0], 28))\n",
    "#for line in X_train:\n",
    "#    X_test_reduced[line] = (np.sum(line.reshape(-1, 28),axis=1))\n",
    "#print(X_test.shape, y_test_new.shape)\n",
    "X_test = np.loadtxt('x_test.txt')\n",
    "Y_test = np.loadtxt('y_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Se prueban los valores y se normaliza el output para poner encontrar cuales cazan con el valor esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = forward_propagate(X_test)\n",
    "activated = test_values['Activation']\n",
    "activated_norm = np.zeros(activated.shape)\n",
    "for i in range(len(activated)):\n",
    "    activated_norm[i] = activated[i]/np.amax(activated[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_2_mnist",
   "language": "python",
   "name": "lab_2_mnist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
